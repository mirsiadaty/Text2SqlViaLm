{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a951a39d-125f-4d81-b536-9c8edd2d4108",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47617bd0-dde6-4315-b9e9-632b942b78e6",
   "metadata": {},
   "source": [
    "\n",
    "# Enlisgh text to SQL code\n",
    "# via LM\n",
    "## \"Qwen2.5-Coder-32B-Instruct-Q8_0.gguf\"\n",
    "## 20250204\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b35fe21-9243-48f4-8e42-afde8f56297e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8759ed8-23fa-4602-bafe-1c07d924b1f7",
   "metadata": {},
   "source": [
    "# download NN file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e9abb02-6c55-40b5-bc57-c159831f5869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Feb  4 12:39:39 2025\n",
      "--2025-02-04 12:39:39--  https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-Q8_0.gguf\n",
      "Resolving huggingface.co (huggingface.co)... 18.244.202.68, 18.244.202.118, 18.244.202.73, ...\n",
      "Connecting to huggingface.co (huggingface.co)|18.244.202.68|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://cdn-lfs-us-1.hf.co/repos/41/1f/411fb0b9655ba8e7fdc41b186f7bd6b5acf7c9baff8892c0a4d23e382fab3936/5e9f8a595c3be1678e3c9a2df19f17e1f5fdd9f2e7b47fcda7b21799c047f575?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Qwen2.5-Coder-32B-Instruct-Q8_0.gguf%3B+filename%3D%22Qwen2.5-Coder-32B-Instruct-Q8_0.gguf%22%3B&Expires=1738694379&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczODY5NDM3OX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzQxLzFmLzQxMWZiMGI5NjU1YmE4ZTdmZGM0MWIxODZmN2JkNmI1YWNmN2M5YmFmZjg4OTJjMGE0ZDIzZTM4MmZhYjM5MzYvNWU5ZjhhNTk1YzNiZTE2NzhlM2M5YTJkZjE5ZjE3ZTFmNWZkZDlmMmU3YjQ3ZmNkYTdiMjE3OTljMDQ3ZjU3NT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=akWD-U5ZrghHifI%7E29UZe8l4PjVrpFFfa1xiftCzrW4JMJWquFODa0LHYAYRs2MsfGduiM83vFWBzm9P-VCG6r4JfdVlfAZ8JspHa15rKYXoBv%7EhGRZtP0iQ9B2p8tWnDZuHTLnRLIDCxhRL3PDXjEy0oHJJN%7Eq0m3EpR6Cvz96iD42PYZEMxiGZJzZ2gL7P9NqVvCBC913pttdoK52EIR8GR6WAsGTAfnrEXI7P%7EwaYMXDRmL-wUVioZPJJM2mu4mRofOJ4PQuC9jB4P3u3I6DHSe2fAO2xJNlTS05jnPTMu2Y0ZKvnjas0Z1JNcUGw55C402LSPnm93-JJMa8T3A__&Key-Pair-Id=K24J24Z295AEI9 [following]\n",
      "--2025-02-04 12:39:39--  https://cdn-lfs-us-1.hf.co/repos/41/1f/411fb0b9655ba8e7fdc41b186f7bd6b5acf7c9baff8892c0a4d23e382fab3936/5e9f8a595c3be1678e3c9a2df19f17e1f5fdd9f2e7b47fcda7b21799c047f575?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Qwen2.5-Coder-32B-Instruct-Q8_0.gguf%3B+filename%3D%22Qwen2.5-Coder-32B-Instruct-Q8_0.gguf%22%3B&Expires=1738694379&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczODY5NDM3OX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzQxLzFmLzQxMWZiMGI5NjU1YmE4ZTdmZGM0MWIxODZmN2JkNmI1YWNmN2M5YmFmZjg4OTJjMGE0ZDIzZTM4MmZhYjM5MzYvNWU5ZjhhNTk1YzNiZTE2NzhlM2M5YTJkZjE5ZjE3ZTFmNWZkZDlmMmU3YjQ3ZmNkYTdiMjE3OTljMDQ3ZjU3NT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=akWD-U5ZrghHifI%7E29UZe8l4PjVrpFFfa1xiftCzrW4JMJWquFODa0LHYAYRs2MsfGduiM83vFWBzm9P-VCG6r4JfdVlfAZ8JspHa15rKYXoBv%7EhGRZtP0iQ9B2p8tWnDZuHTLnRLIDCxhRL3PDXjEy0oHJJN%7Eq0m3EpR6Cvz96iD42PYZEMxiGZJzZ2gL7P9NqVvCBC913pttdoK52EIR8GR6WAsGTAfnrEXI7P%7EwaYMXDRmL-wUVioZPJJM2mu4mRofOJ4PQuC9jB4P3u3I6DHSe2fAO2xJNlTS05jnPTMu2Y0ZKvnjas0Z1JNcUGw55C402LSPnm93-JJMa8T3A__&Key-Pair-Id=K24J24Z295AEI9\n",
      "18.164.78.6, 18.164.78.101, 18.164.78.17, ....co)... \n",
      "Connecting to cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)|18.164.78.6|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 34820885184 (32G) [binary/octet-stream]\n",
      "Saving to: ‘Qwen2.5-Coder-32B-Instruct-Q8_0.gguf’\n",
      "\n",
      "Qwen2.5-Coder-32B-I 100%[===================>]  32.43G  38.8MB/s    in 13m 50s \n",
      "\n",
      "2025-02-04 12:53:29 (40.0 MB/s) - ‘Qwen2.5-Coder-32B-Instruct-Q8_0.gguf’ saved [34820885184/34820885184]\n",
      "\n",
      "Tue Feb  4 12:53:29 2025\n",
      "Tue Feb  4 12:53:29 2025\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(time.asctime( time.localtime( time.time() ) ))\n",
    "\n",
    "\n",
    "# MssJupy_TryLlm_Qwen2.5coder_202502041237.ipynb\n",
    "#https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/blob/main/Qwen2.5-Coder-32B-Instruct-Q8_0.gguf\n",
    "!wget https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-Q8_0.gguf\n",
    "print(time.asctime( time.localtime( time.time() ) ))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(time.asctime( time.localtime( time.time() ) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486e7ee9-c2d5-4555-ac75-fc9b66697cf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e99bd40-71cb-4826-917f-e425ec3732ef",
   "metadata": {},
   "source": [
    "# load lm to ram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40d61481-786b-4d7a-8133-8f43620ae45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 Tue Feb  4 12:55:46 2025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
      "ggml_cuda_init: found 2 CUDA devices:\n",
      "  Device 0: Quadro RTX 8000, compute capability 7.5, VMM: yes\n",
      "  Device 1: Quadro RTX 8000, compute capability 7.5, VMM: yes\n",
      "llama_model_load_from_file_impl: using device CUDA0 (Quadro RTX 8000) - 48344 MiB free\n",
      "llama_model_load_from_file_impl: using device CUDA1 (Quadro RTX 8000) - 47203 MiB free\n",
      "llama_model_loader: loaded meta data with 34 key-value pairs and 771 tensors from Qwen2.5-Coder-32B-Instruct-Q8_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 32B Instruct\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 32B\n",
      "llama_model_loader: - kv   6:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...\n",
      "llama_model_loader: - kv   8:                   general.base_model.count u32              = 1\n",
      "llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 32B\n",
      "llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen\n",
      "llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...\n",
      "llama_model_loader: - kv  12:                               general.tags arr[str,6]       = [\"code\", \"codeqwen\", \"chat\", \"qwen\", ...\n",
      "llama_model_loader: - kv  13:                          general.languages arr[str,1]       = [\"en\"]\n",
      "llama_model_loader: - kv  14:                          qwen2.block_count u32              = 64\n",
      "llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768\n",
      "llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 27648\n",
      "llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  22:                          general.file_type u32              = 7\n",
      "llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2\n",
      "llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
      "llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645\n",
      "llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643\n",
      "llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643\n",
      "llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\n",
      "llama_model_loader: - kv  33:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  321 tensors\n",
      "llama_model_loader: - type q8_0:  450 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q8_0\n",
      "print_info: file size   = 32.42 GiB (8.50 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 2\n",
      "load: control token: 151660 '<|fim_middle|>' is not marked as EOG\n",
      "load: control token: 151659 '<|fim_prefix|>' is not marked as EOG\n",
      "load: control token: 151653 '<|vision_end|>' is not marked as EOG\n",
      "load: control token: 151648 '<|box_start|>' is not marked as EOG\n",
      "load: control token: 151646 '<|object_ref_start|>' is not marked as EOG\n",
      "load: control token: 151649 '<|box_end|>' is not marked as EOG\n",
      "load: control token: 151655 '<|image_pad|>' is not marked as EOG\n",
      "load: control token: 151651 '<|quad_end|>' is not marked as EOG\n",
      "load: control token: 151647 '<|object_ref_end|>' is not marked as EOG\n",
      "load: control token: 151652 '<|vision_start|>' is not marked as EOG\n",
      "load: control token: 151654 '<|vision_pad|>' is not marked as EOG\n",
      "load: control token: 151656 '<|video_pad|>' is not marked as EOG\n",
      "load: control token: 151644 '<|im_start|>' is not marked as EOG\n",
      "load: control token: 151661 '<|fim_suffix|>' is not marked as EOG\n",
      "load: control token: 151650 '<|quad_start|>' is not marked as EOG\n",
      "load: special tokens cache size = 22\n",
      "load: token to piece cache size = 0.9310 MB\n",
      "print_info: arch             = qwen2\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 32768\n",
      "print_info: n_embd           = 5120\n",
      "print_info: n_layer          = 64\n",
      "print_info: n_head           = 40\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 5\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-06\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: n_ff             = 27648\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 2\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 1000000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 32768\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 32B\n",
      "print_info: model params     = 32.76 B\n",
      "print_info: general.name     = Qwen2.5 Coder 32B Instruct\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 152064\n",
      "print_info: n_merges         = 151387\n",
      "print_info: BOS token        = 151643 '<|endoftext|>'\n",
      "print_info: EOS token        = 151645 '<|im_end|>'\n",
      "print_info: EOT token        = 151645 '<|im_end|>'\n",
      "print_info: PAD token        = 151643 '<|endoftext|>'\n",
      "print_info: LF token         = 148848 'ÄĬ'\n",
      "print_info: FIM PRE token    = 151659 '<|fim_prefix|>'\n",
      "print_info: FIM SUF token    = 151661 '<|fim_suffix|>'\n",
      "print_info: FIM MID token    = 151660 '<|fim_middle|>'\n",
      "print_info: FIM PAD token    = 151662 '<|fim_pad|>'\n",
      "print_info: FIM REP token    = 151663 '<|repo_name|>'\n",
      "print_info: FIM SEP token    = 151664 '<|file_sep|>'\n",
      "print_info: EOG token        = 151643 '<|endoftext|>'\n",
      "print_info: EOG token        = 151645 '<|im_end|>'\n",
      "print_info: EOG token        = 151662 '<|fim_pad|>'\n",
      "print_info: EOG token        = 151663 '<|repo_name|>'\n",
      "print_info: EOG token        = 151664 '<|file_sep|>'\n",
      "print_info: max token length = 256\n",
      "load_tensors: layer   0 assigned to device CUDA0\n",
      "load_tensors: layer   1 assigned to device CUDA0\n",
      "load_tensors: layer   2 assigned to device CUDA0\n",
      "load_tensors: layer   3 assigned to device CUDA0\n",
      "load_tensors: layer   4 assigned to device CUDA0\n",
      "load_tensors: layer   5 assigned to device CUDA0\n",
      "load_tensors: layer   6 assigned to device CUDA0\n",
      "load_tensors: layer   7 assigned to device CUDA0\n",
      "load_tensors: layer   8 assigned to device CUDA0\n",
      "load_tensors: layer   9 assigned to device CUDA0\n",
      "load_tensors: layer  10 assigned to device CUDA0\n",
      "load_tensors: layer  11 assigned to device CUDA0\n",
      "load_tensors: layer  12 assigned to device CUDA0\n",
      "load_tensors: layer  13 assigned to device CUDA0\n",
      "load_tensors: layer  14 assigned to device CUDA0\n",
      "load_tensors: layer  15 assigned to device CUDA0\n",
      "load_tensors: layer  16 assigned to device CUDA0\n",
      "load_tensors: layer  17 assigned to device CUDA0\n",
      "load_tensors: layer  18 assigned to device CUDA0\n",
      "load_tensors: layer  19 assigned to device CUDA0\n",
      "load_tensors: layer  20 assigned to device CUDA0\n",
      "load_tensors: layer  21 assigned to device CUDA0\n",
      "load_tensors: layer  22 assigned to device CUDA0\n",
      "load_tensors: layer  23 assigned to device CUDA0\n",
      "load_tensors: layer  24 assigned to device CUDA0\n",
      "load_tensors: layer  25 assigned to device CUDA0\n",
      "load_tensors: layer  26 assigned to device CUDA0\n",
      "load_tensors: layer  27 assigned to device CUDA0\n",
      "load_tensors: layer  28 assigned to device CUDA0\n",
      "load_tensors: layer  29 assigned to device CUDA0\n",
      "load_tensors: layer  30 assigned to device CUDA0\n",
      "load_tensors: layer  31 assigned to device CUDA0\n",
      "load_tensors: layer  32 assigned to device CUDA0\n",
      "load_tensors: layer  33 assigned to device CUDA1\n",
      "load_tensors: layer  34 assigned to device CUDA1\n",
      "load_tensors: layer  35 assigned to device CUDA1\n",
      "load_tensors: layer  36 assigned to device CUDA1\n",
      "load_tensors: layer  37 assigned to device CUDA1\n",
      "load_tensors: layer  38 assigned to device CUDA1\n",
      "load_tensors: layer  39 assigned to device CUDA1\n",
      "load_tensors: layer  40 assigned to device CUDA1\n",
      "load_tensors: layer  41 assigned to device CUDA1\n",
      "load_tensors: layer  42 assigned to device CUDA1\n",
      "load_tensors: layer  43 assigned to device CUDA1\n",
      "load_tensors: layer  44 assigned to device CUDA1\n",
      "load_tensors: layer  45 assigned to device CUDA1\n",
      "load_tensors: layer  46 assigned to device CUDA1\n",
      "load_tensors: layer  47 assigned to device CUDA1\n",
      "load_tensors: layer  48 assigned to device CUDA1\n",
      "load_tensors: layer  49 assigned to device CUDA1\n",
      "load_tensors: layer  50 assigned to device CUDA1\n",
      "load_tensors: layer  51 assigned to device CUDA1\n",
      "load_tensors: layer  52 assigned to device CUDA1\n",
      "load_tensors: layer  53 assigned to device CUDA1\n",
      "load_tensors: layer  54 assigned to device CUDA1\n",
      "load_tensors: layer  55 assigned to device CUDA1\n",
      "load_tensors: layer  56 assigned to device CUDA1\n",
      "load_tensors: layer  57 assigned to device CUDA1\n",
      "load_tensors: layer  58 assigned to device CUDA1\n",
      "load_tensors: layer  59 assigned to device CUDA1\n",
      "load_tensors: layer  60 assigned to device CUDA1\n",
      "load_tensors: layer  61 assigned to device CUDA1\n",
      "load_tensors: layer  62 assigned to device CUDA1\n",
      "load_tensors: layer  63 assigned to device CUDA1\n",
      "load_tensors: layer  64 assigned to device CUDA1\n",
      "load_tensors: tensor 'token_embd.weight' (q8_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors: offloading 64 repeating layers to GPU\n",
      "load_tensors: offloading output layer to GPU\n",
      "load_tensors: offloaded 65/65 layers to GPU\n",
      "load_tensors:        CUDA0 model buffer size = 16306.25 MiB\n",
      "load_tensors:        CUDA1 model buffer size = 16106.92 MiB\n",
      "load_tensors:   CPU_Mapped model buffer size =   788.91 MiB\n",
      "llama_init_from_model: n_seq_max     = 1\n",
      "llama_init_from_model: n_ctx         = 10016\n",
      "llama_init_from_model: n_ctx_per_seq = 10016\n",
      "llama_init_from_model: n_batch       = 512\n",
      "llama_init_from_model: n_ubatch      = 512\n",
      "llama_init_from_model: flash_attn    = 0\n",
      "llama_init_from_model: freq_base     = 1000000.0\n",
      "llama_init_from_model: freq_scale    = 1\n",
      "llama_init_from_model: n_ctx_per_seq (10016) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_init: kv_size = 10016, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 64, can_shift = 1\n",
      "llama_kv_cache_init: layer 0: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 1: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 2: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 3: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 4: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 5: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 6: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 7: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 8: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 9: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 10: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 11: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 12: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 13: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 14: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 15: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 16: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 17: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 18: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 19: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 20: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 21: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 22: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 23: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 24: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 25: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 26: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 27: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 28: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 29: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 30: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 31: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 32: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 33: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 34: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 35: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 36: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 37: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 38: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 39: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 40: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 41: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 42: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 43: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 44: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 45: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 46: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 47: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 48: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 49: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 50: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 51: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 52: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 53: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 54: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 55: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 56: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 57: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 58: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 59: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 60: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 61: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 62: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 63: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =  1291.12 MiB\n",
      "llama_kv_cache_init:      CUDA1 KV buffer size =  1212.88 MiB\n",
      "llama_init_from_model: KV self size  = 2504.00 MiB, K (f16): 1252.00 MiB, V (f16): 1252.00 MiB\n",
      "llama_init_from_model:  CUDA_Host  output buffer size =     0.58 MiB\n",
      "llama_init_from_model: pipeline parallelism enabled (n_copies=4)\n",
      "llama_init_from_model:      CUDA0 compute buffer size =   940.76 MiB\n",
      "llama_init_from_model:      CUDA1 compute buffer size =   940.77 MiB\n",
      "llama_init_from_model:  CUDA_Host compute buffer size =    88.27 MiB\n",
      "llama_init_from_model: graph nodes  = 2246\n",
      "llama_init_from_model: graph splits = 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 11.548148155212402 seconds to run end-to-end.\n",
      "99 Tue Feb  4 12:55:57 2025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA : ARCHS = 520,610,700,750 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | AVX512 = 1 | AVX512_VNNI = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'tokenizer.ggml.bos_token_id': '151643', 'general.file_type': '7', 'qwen2.attention.layer_norm_rms_epsilon': '0.000001', 'tokenizer.ggml.eos_token_id': '151645', 'qwen2.rope.freq_base': '1000000.000000', 'qwen2.attention.head_count': '40', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'qwen2.feed_forward_length': '27648', 'qwen2.context_length': '32768', 'general.architecture': 'qwen2', 'tokenizer.ggml.padding_token_id': '151643', 'qwen2.embedding_length': '5120', 'general.basename': 'Qwen2.5-Coder', 'tokenizer.ggml.add_bos_token': 'false', 'general.base_model.0.organization': 'Qwen', 'tokenizer.ggml.pre': 'qwen2', 'general.name': 'Qwen2.5 Coder 32B Instruct', 'general.base_model.0.name': 'Qwen2.5 Coder 32B', 'qwen2.block_count': '64', 'general.finetune': 'Instruct', 'general.type': 'model', 'general.size_label': '32B', 'tokenizer.chat_template': '{%- if tools %}\\n    {{- \\'<|im_start|>system\\\\n\\' }}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- messages[0][\\'content\\'] }}\\n    {%- else %}\\n        {{- \\'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\\' }}\\n    {%- endif %}\\n    {{- \"\\\\n\\\\n# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\" }}\\n    {%- for tool in tools %}\\n        {{- \"\\\\n\" }}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- \"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{\\\\\"name\\\\\": <function-name>, \\\\\"arguments\\\\\": <args-json-object>}\\\\n</tool_call><|im_end|>\\\\n\" }}\\n{%- else %}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- \\'<|im_start|>system\\\\n\\' + messages[0][\\'content\\'] + \\'<|im_end|>\\\\n\\' }}\\n    {%- else %}\\n        {{- \\'<|im_start|>system\\\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\\\n\\' }}\\n    {%- endif %}\\n{%- endif %}\\n{%- for message in messages %}\\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\\n        {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\' + message.content + \\'<|im_end|>\\' + \\'\\\\n\\' }}\\n    {%- elif message.role == \"assistant\" %}\\n        {{- \\'<|im_start|>\\' + message.role }}\\n        {%- if message.content %}\\n            {{- \\'\\\\n\\' + message.content }}\\n        {%- endif %}\\n        {%- for tool_call in message.tool_calls %}\\n            {%- if tool_call.function is defined %}\\n                {%- set tool_call = tool_call.function %}\\n            {%- endif %}\\n            {{- \\'\\\\n<tool_call>\\\\n{\"name\": \"\\' }}\\n            {{- tool_call.name }}\\n            {{- \\'\", \"arguments\": \\' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- \\'}\\\\n</tool_call>\\' }}\\n        {%- endfor %}\\n        {{- \\'<|im_end|>\\\\n\\' }}\\n    {%- elif message.role == \"tool\" %}\\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\\n            {{- \\'<|im_start|>user\\' }}\\n        {%- endif %}\\n        {{- \\'\\\\n<tool_response>\\\\n\\' }}\\n        {{- message.content }}\\n        {{- \\'\\\\n</tool_response>\\' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\\n            {{- \\'<|im_end|>\\\\n\\' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|im_start|>assistant\\\\n\\' }}\\n{%- endif %}\\n', 'qwen2.attention.head_count_kv': '8', 'general.license.link': 'https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct/blob/main/LICENSE', 'general.base_model.count': '1', 'general.license': 'apache-2.0', 'general.base_model.0.repo_url': 'https://huggingface.co/Qwen/Qwen2.5-Coder-32B'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {%- if tools %}\n",
      "    {{- '<|im_start|>system\\n' }}\n",
      "    {%- if messages[0]['role'] == 'system' %}\n",
      "        {{- messages[0]['content'] }}\n",
      "    {%- else %}\n",
      "        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\n",
      "    {%- endif %}\n",
      "    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n",
      "    {%- for tool in tools %}\n",
      "        {{- \"\\n\" }}\n",
      "        {{- tool | tojson }}\n",
      "    {%- endfor %}\n",
      "    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n",
      "{%- else %}\n",
      "    {%- if messages[0]['role'] == 'system' %}\n",
      "        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n",
      "    {%- else %}\n",
      "        {{- '<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n' }}\n",
      "    {%- endif %}\n",
      "{%- endif %}\n",
      "{%- for message in messages %}\n",
      "    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n",
      "        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n",
      "    {%- elif message.role == \"assistant\" %}\n",
      "        {{- '<|im_start|>' + message.role }}\n",
      "        {%- if message.content %}\n",
      "            {{- '\\n' + message.content }}\n",
      "        {%- endif %}\n",
      "        {%- for tool_call in message.tool_calls %}\n",
      "            {%- if tool_call.function is defined %}\n",
      "                {%- set tool_call = tool_call.function %}\n",
      "            {%- endif %}\n",
      "            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n",
      "            {{- tool_call.name }}\n",
      "            {{- '\", \"arguments\": ' }}\n",
      "            {{- tool_call.arguments | tojson }}\n",
      "            {{- '}\\n</tool_call>' }}\n",
      "        {%- endfor %}\n",
      "        {{- '<|im_end|>\\n' }}\n",
      "    {%- elif message.role == \"tool\" %}\n",
      "        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n",
      "            {{- '<|im_start|>user' }}\n",
      "        {%- endif %}\n",
      "        {{- '\\n<tool_response>\\n' }}\n",
      "        {{- message.content }}\n",
      "        {{- '\\n</tool_response>' }}\n",
      "        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n",
      "            {{- '<|im_end|>\\n' }}\n",
      "        {%- endif %}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|im_start|>assistant\\n' }}\n",
      "{%- endif %}\n",
      "\n",
      "Using chat eos_token: <|im_end|>\n",
      "Using chat bos_token: <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## parms\n",
    "PRMModelID11 = 'Qwen2.5-Coder-32B-Instruct-Q8_0.gguf'\n",
    "PRMContextTokenCount = 9999\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "# e2e time\n",
    "start00 = time.time()\n",
    "print('11' , time.asctime( time.localtime( time.time() ) ))\n",
    "\n",
    "\n",
    "\n",
    "from llama_cpp import Llama\n",
    "\n",
    "\n",
    "### parms\n",
    "\n",
    "# MssJupy_IRviaLucMariaLlm_202409081135.ipynb : make rerank llm prompt more similar to the later QuestionAnswer prompt.\n",
    "ContextTokenCount = PRMContextTokenCount\n",
    "\n",
    "# MssJupy_IRviaLucMariaLlm_202409151623.ipynb : mixture of llm responses.\n",
    "ModelID = PRMModelID11\n",
    "\n",
    "## load nn to ram\n",
    "llm101 = Llama(model_path=ModelID, n_gpu_layers=-1, n_ctx=ContextTokenCount )\n",
    "\n",
    "\n",
    "\n",
    "# e2e time\n",
    "end00 = time.time()\n",
    "#print(\"Took {} seconds to pull {} websites.\".format(end11 - start11, len(work101)))\n",
    "print(\"Took {} seconds to run end-to-end.\".format(end00 - start00 ))\n",
    "#\n",
    "TimeTookE2E = end00 - start00\n",
    "\n",
    "\n",
    "print( '99' , time.asctime( time.localtime( time.time() ) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da75ac65-8cab-456b-9785-3c0800fcd8aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3127ff9-cd7d-44ed-99d1-2dd2224c9e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               total        used        free      shared  buff/cache   available\n",
      "Mem:           1.1Ti       7.8Gi       1.0Ti       160Mi       111Gi       1.1Ti\n",
      "Swap:          2.0Gi          0B       2.0Gi\n",
      "Total:         1.1Ti       7.8Gi       1.0Ti\n",
      "Tue Feb  4 12:55:57 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Quadro RTX 8000                Off | 00000000:3B:00.0 Off |                  Off |\n",
      "| 33%   26C    P2              66W / 260W |  18790MiB / 49152MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  Quadro RTX 8000                Off | 00000000:AF:00.0  On |                  Off |\n",
      "| 33%   35C    P2              90W / 260W |  19644MiB / 49152MiB |      9%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      3599      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "|    0   N/A  N/A    159208      C   ...e/ghtw30s/virenv20240420/bin/python    18782MiB |\n",
      "|    1   N/A  N/A      3599      G   /usr/lib/xorg/Xorg                          654MiB |\n",
      "|    1   N/A  N/A      3874      G   /usr/bin/gnome-shell                        114MiB |\n",
      "|    1   N/A  N/A      6445      G   ...irefox/5647/usr/lib/firefox/firefox      221MiB |\n",
      "|    1   N/A  N/A    159208      C   ...e/ghtw30s/virenv20240420/bin/python    18504MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# record ram usage cpu gpu\n",
    "!free -g -h -t\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90ae815-0143-44d5-87e9-f337ccc7347e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3eb094-91a8-459d-be2d-c7fc5bca8bb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aef477f1-b3f5-42fd-b59e-e61cb39731d2",
   "metadata": {},
   "source": [
    "# read prepared trainset from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa43832c-d5db-4689-bd6e-78b05fbb11d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 ghtw30s ghtw30s     157 Aug 27 10:40 mnfst33.json\n",
      "-rw-rw-r-- 1 ghtw30s ghtw30s 1182876 Feb 10  2024 test_dataset.json\n",
      "-rw-rw-r-- 1 ghtw30s ghtw30s 4755289 Feb 10  2024 train_dataset.json\n"
     ]
    }
   ],
   "source": [
    "!ls -ltA *.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "96a3053d-c08d-4e2a-a096-e9f22a2bfc4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Feb  4 13:39:10 2025\n",
      "Tue Feb  4 13:39:10 2025\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "print(time.asctime( time.localtime( time.time() ) ))\n",
    "\n",
    "# Load jsonl data from disk\n",
    "dataset = load_dataset(\"json\", data_files=\"train_dataset.json\", split=\"train\")\n",
    "\n",
    "\n",
    "print(time.asctime( time.localtime( time.time() ) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a32a85cf-8b63-4621-8cbc-ed174b56ce89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['messages'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# mss: inspect data format sent to llm for training\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0efc156e-8d22-432e-ac23-13410e0755c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [[{'content': 'You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\\nSCHEMA:\\nCREATE TABLE table_name_90 (team_1 VARCHAR)',\n",
       "    'role': 'system'},\n",
       "   {'content': 'Name the 2nd leg for team 1 of hamburg', 'role': 'user'},\n",
       "   {'content': 'SELECT 2 AS nd_leg FROM table_name_90 WHERE team_1 = \"hamburg\"',\n",
       "    'role': 'assistant'}],\n",
       "  [{'content': 'You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\\nSCHEMA:\\nCREATE TABLE table_name_54 (season VARCHAR, lead VARCHAR, third VARCHAR)',\n",
       "    'role': 'system'},\n",
       "   {'content': 'what is the season when the lead is john shuster and third is shawn rojeski?',\n",
       "    'role': 'user'},\n",
       "   {'content': 'SELECT season FROM table_name_54 WHERE lead = \"john shuster\" AND third = \"shawn rojeski\"',\n",
       "    'role': 'assistant'}],\n",
       "  [{'content': 'You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\\nSCHEMA:\\nCREATE TABLE table_name_75 (category VARCHAR, director VARCHAR)',\n",
       "    'role': 'system'},\n",
       "   {'content': 'Tell me the category of na director', 'role': 'user'},\n",
       "   {'content': 'SELECT category FROM table_name_75 WHERE director = \"na\"',\n",
       "    'role': 'assistant'}],\n",
       "  [{'content': 'You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\\nSCHEMA:\\nCREATE TABLE table_name_25 (directed___undirected VARCHAR, induced___non_induced VARCHAR, name VARCHAR)',\n",
       "    'role': 'system'},\n",
       "   {'content': 'What is the directed/undirected of fpf (mavisto), which has an induced/non-induced of induced?',\n",
       "    'role': 'user'},\n",
       "   {'content': 'SELECT directed___undirected FROM table_name_25 WHERE induced___non_induced = \"induced\" AND name = \"fpf (mavisto)\"',\n",
       "    'role': 'assistant'}],\n",
       "  [{'content': 'You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\\nSCHEMA:\\nCREATE TABLE table_name_10 (poll VARCHAR, wk_13 VARCHAR, wk_10 VARCHAR, wk_2 VARCHAR)',\n",
       "    'role': 'system'},\n",
       "   {'content': 'Which poll had a week 10 larger than 2, a week 2 of exactly 12, and a week 13 of 8?',\n",
       "    'role': 'user'},\n",
       "   {'content': 'SELECT poll FROM table_name_10 WHERE wk_10 > 2 AND wk_2 = \"12\" AND wk_13 = 8',\n",
       "    'role': 'assistant'}]]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[34:39]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8321c2-45dc-4eab-8727-56bce451688f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b56c0bb5-a995-467e-adcd-abcadcd84730",
   "metadata": {},
   "source": [
    "# ask lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2bcfbb3e-d06e-4ee1-a02e-399d6500699b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numbers [4932, 9214, 2318, 4824, 5780, 7628, 7494, 326, 6531, 1353, 1355, 4530, 9735, 3963, 8922, 3576, 2286, 7242, 8417, 590]\n",
      "====================================================================\n",
      "\n",
      "\n",
      "22 Tue Feb  4 13:48:26 2025\n",
      "iter 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 41 prefix-match hit, remaining 32 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1612.26 ms\n",
      "llama_perf_context_print: prompt eval time =     168.77 ms /    32 tokens (    5.27 ms per token,   189.60 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6206.55 ms /    94 runs   (   66.03 ms per token,    15.15 tokens per second)\n",
      "llama_perf_context_print:       total time =    6509.67 ms /   126 tokens\n",
      "Llama.generate: 41 prefix-match hit, remaining 32 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\n",
      "SCHEMA:\n",
      "CREATE TABLE table_name_59 (runner_up VARCHAR, host VARCHAR)<|im_end|>\n",
      "<|im_start|>user\n",
      "What was the Runner-up in the Monroe County Sports Commission?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "To generate an SQL query for this question, we need to assume that \"Monroe County Sports Commission\" is the host. Based on the provided schema, the query would look like this:\n",
      "\n",
      "```sql\n",
      "SELECT runner_up\n",
      "FROM table_name_59\n",
      "WHERE host = 'Monroe County Sports Commission';\n",
      "```\n",
      "\n",
      "This query selects the `runner_up` from the `table_name_59` where the `host` is 'Monroe County Sports Commission'.\n",
      "----------------------------------\n",
      "Gold Standard:\n",
      " SELECT runner_up FROM table_name_59 WHERE host = \"monroe county sports commission\"\n",
      "----------------------------------\n",
      "Took 11.548148155212402 seconds.\n",
      "====================================================================\n",
      "\n",
      "\n",
      "iter 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1612.26 ms\n",
      "llama_perf_context_print: prompt eval time =      92.99 ms /    32 tokens (    2.91 ms per token,   344.13 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4122.84 ms /    62 runs   (   66.50 ms per token,    15.04 tokens per second)\n",
      "llama_perf_context_print:       total time =    4300.91 ms /    94 tokens\n",
      "Llama.generate: 38 prefix-match hit, remaining 38 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\n",
      "SCHEMA:\n",
      "CREATE TABLE table_name_18 (score INTEGER, player VARCHAR)<|im_end|>\n",
      "<|im_start|>user\n",
      "What is Sum of Score, when Player is Vijay Singh?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "To find the sum of the scores for the player \"Vijay Singh\" from the table `table_name_18`, you can use the following SQL query:\n",
      "\n",
      "```sql\n",
      "SELECT SUM(score) AS total_score\n",
      "FROM table_name_18\n",
      "WHERE player = 'Vijay Singh';\n",
      "```\n",
      "----------------------------------\n",
      "Gold Standard:\n",
      " SELECT SUM(score) FROM table_name_18 WHERE player = \"vijay singh\"\n",
      "----------------------------------\n",
      "Took 11.548148155212402 seconds.\n",
      "====================================================================\n",
      "\n",
      "\n",
      "iter 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1612.26 ms\n",
      "llama_perf_context_print: prompt eval time =      98.45 ms /    38 tokens (    2.59 ms per token,   385.98 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6493.49 ms /   100 runs   (   64.93 ms per token,    15.40 tokens per second)\n",
      "llama_perf_context_print:       total time =    6730.35 ms /   138 tokens\n",
      "Llama.generate: 38 prefix-match hit, remaining 35 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\n",
      "SCHEMA:\n",
      "CREATE TABLE employee (emp_fname VARCHAR, emp_lname VARCHAR, emp_dob VARCHAR)<|im_end|>\n",
      "<|im_start|>user\n",
      "What is the first and last name of the oldest employee?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "To find the first and last name of the oldest employee, you can use the following SQL query:\n",
      "\n",
      "```sql\n",
      "SELECT emp_fname, emp_lname\n",
      "FROM employee\n",
      "ORDER BY emp_dob ASC\n",
      "LIMIT 1;\n",
      "```\n",
      "\n",
      "This query orders the employees by their date of birth in ascending order, so the oldest employee (with the earliest date of birth) comes first. The `LIMIT 1` clause ensures that only the first record (i.e., the oldest employee) is returned.\n",
      "----------------------------------\n",
      "Gold Standard:\n",
      " SELECT emp_fname, emp_lname FROM employee ORDER BY emp_dob LIMIT 1\n",
      "----------------------------------\n",
      "Took 11.548148155212402 seconds.\n",
      "====================================================================\n",
      "\n",
      "\n",
      "iter 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1612.26 ms\n",
      "llama_perf_context_print: prompt eval time =      97.71 ms /    35 tokens (    2.79 ms per token,   358.21 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3080.03 ms /    47 runs   (   65.53 ms per token,    15.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    3253.77 ms /    82 tokens\n",
      "Llama.generate: 39 prefix-match hit, remaining 51 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\n",
      "SCHEMA:\n",
      "CREATE TABLE table_name_5 (artist VARCHAR, percentage VARCHAR)<|im_end|>\n",
      "<|im_start|>user\n",
      "What Artist had a Percentage of 17.14%?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "To find the artist with a percentage of 17.14%, you can use the following SQL query:\n",
      "\n",
      "```sql\n",
      "SELECT artist\n",
      "FROM table_name_5\n",
      "WHERE percentage = '17.14%';\n",
      "```\n",
      "----------------------------------\n",
      "Gold Standard:\n",
      " SELECT artist FROM table_name_5 WHERE percentage = \"17.14%\"\n",
      "----------------------------------\n",
      "Took 11.548148155212402 seconds.\n",
      "====================================================================\n",
      "\n",
      "\n",
      "iter 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1612.26 ms\n",
      "llama_perf_context_print: prompt eval time =     116.22 ms /    51 tokens (    2.28 ms per token,   438.81 tokens per second)\n",
      "llama_perf_context_print:        eval time =   12584.02 ms /   191 runs   (   65.88 ms per token,    15.18 tokens per second)\n",
      "llama_perf_context_print:       total time =   12991.00 ms /   242 tokens\n",
      "Llama.generate: 39 prefix-match hit, remaining 50 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\n",
      "SCHEMA:\n",
      "CREATE TABLE table_197446_1 (elected VARCHAR, term_began VARCHAR)<|im_end|>\n",
      "<|im_start|>user\n",
      "What year was the congressional service elected for a term beginning on January 3, 1989?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "To find the year of the congressional service for a term beginning on January 3, 1989, you can use the following SQL query:\n",
      "\n",
      "```sql\n",
      "SELECT term_began\n",
      "FROM table_197446_1\n",
      "WHERE term_began = '1989-01-03';\n",
      "```\n",
      "\n",
      "This query selects the `term_began` column from `table_197446_1` where the `term_began` date matches January 3, 1989. If you specifically need the year, you can extract it using the `YEAR` function:\n",
      "\n",
      "```sql\n",
      "SELECT YEAR(term_began) AS year_of_service\n",
      "FROM table_197446_1\n",
      "WHERE term_began = '1989-01-03';\n",
      "```\n",
      "\n",
      "This will return the year 1989 as the year of service.\n",
      "----------------------------------\n",
      "Gold Standard:\n",
      " SELECT elected FROM table_197446_1 WHERE term_began = \"January 3, 1989\"\n",
      "----------------------------------\n",
      "Took 11.548148155212402 seconds.\n",
      "====================================================================\n",
      "\n",
      "\n",
      "iter 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1612.26 ms\n",
      "llama_perf_context_print: prompt eval time =     116.33 ms /    50 tokens (    2.33 ms per token,   429.80 tokens per second)\n",
      "llama_perf_context_print:        eval time =   16296.65 ms /   253 runs   (   64.41 ms per token,    15.52 tokens per second)\n",
      "llama_perf_context_print:       total time =   16787.84 ms /   303 tokens\n",
      "Llama.generate: 41 prefix-match hit, remaining 38 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\n",
      "SCHEMA:\n",
      "CREATE TABLE table_name_91 (years VARCHAR, name VARCHAR, area VARCHAR, decile VARCHAR)<|im_end|>\n",
      "<|im_start|>user\n",
      "Name the years for marton and decile of 3 for turakina maori girls' college?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "To generate a SQL query based on your request, we need to select the years for the name 'Marton' and the decile for the name 'Turakina Maori Girls\\' College' where the decile is 3. However, the schema provided does not specify separate entries for 'Marton' and 'Turakina Maori Girls\\' College' in the same table, so I'll assume you want to retrieve these as separate queries or combined in a way that makes sense given the schema.\n",
      "\n",
      "Here's how you can write the queries:\n",
      "\n",
      "1. To get the years for 'Marton':\n",
      "```sql\n",
      "SELECT years\n",
      "FROM table_name_91\n",
      "WHERE name = 'Marton';\n",
      "```\n",
      "\n",
      "2. To get the years and decile for 'Turakina Maori Girls\\' College' where the decile is 3:\n",
      "```sql\n",
      "SELECT years, decile\n",
      "FROM table_name_91\n",
      "WHERE name = 'Turakina Maori Girls'' College' AND decile = '3';\n",
      "```\n",
      "\n",
      "If you want to combine these into a single query, you might need to use a UNION or some other method to handle the different conditions, but based on the schema provided, these are the most straightforward queries.\n",
      "----------------------------------\n",
      "Gold Standard:\n",
      " SELECT years FROM table_name_91 WHERE area = \"marton\" AND decile = 3 AND name = \"turakina maori girls' college\"\n",
      "----------------------------------\n",
      "Took 11.548148155212402 seconds.\n",
      "====================================================================\n",
      "\n",
      "\n",
      "iter 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1612.26 ms\n",
      "llama_perf_context_print: prompt eval time =      99.73 ms /    38 tokens (    2.62 ms per token,   381.04 tokens per second)\n",
      "llama_perf_context_print:        eval time =   11013.48 ms /   172 runs   (   64.03 ms per token,    15.62 tokens per second)\n",
      "llama_perf_context_print:       total time =   11352.02 ms /   210 tokens\n",
      "Llama.generate: 39 prefix-match hit, remaining 34 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\n",
      "SCHEMA:\n",
      "CREATE TABLE table_name_32 (date VARCHAR, goals VARCHAR)<|im_end|>\n",
      "<|im_start|>user\n",
      "Which date has a Goal of deacon 4/4, withers 1 dg?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "To generate a SQL query based on the provided schema and the user's question, we need to assume that the \"goals\" column contains the information about the goals in a specific format. Given the format in the question, it seems like the \"goals\" column might store strings that describe the goals in a certain way.\n",
      "\n",
      "Here's a SQL query that attempts to find the date with the specified goal:\n",
      "\n",
      "```sql\n",
      "SELECT date\n",
      "FROM table_name_32\n",
      "WHERE goals = 'deacon 4/4, withers 1 dg';\n",
      "```\n",
      "\n",
      "This query selects the `date` from `table_name_32` where the `goals` column matches the exact string 'deacon 4/4, withers 1 dg'. If the format of the `goals` column is different, the query might need to be adjusted accordingly.\n",
      "----------------------------------\n",
      "Gold Standard:\n",
      " SELECT date FROM table_name_32 WHERE goals = \"deacon 4/4, withers 1 dg\"\n",
      "----------------------------------\n",
      "Took 11.548148155212402 seconds.\n",
      "====================================================================\n",
      "\n",
      "\n",
      "iter 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1612.26 ms\n",
      "llama_perf_context_print: prompt eval time =      97.30 ms /    34 tokens (    2.86 ms per token,   349.42 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3361.54 ms /    53 runs   (   63.43 ms per token,    15.77 tokens per second)\n",
      "llama_perf_context_print:       total time =    3529.18 ms /    87 tokens\n",
      "Llama.generate: 39 prefix-match hit, remaining 57 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\n",
      "SCHEMA:\n",
      "CREATE TABLE table_226619_12 (pos INTEGER, club VARCHAR)<|im_end|>\n",
      "<|im_start|>user\n",
      "Name the pos for west ham united<|im_end|>\n",
      "<|im_start|>assistant\n",
      "To find the `pos` for \"West Ham United\" from the given table, you can use the following SQL query:\n",
      "\n",
      "```sql\n",
      "SELECT pos\n",
      "FROM table_226619_12\n",
      "WHERE club = 'West Ham United';\n",
      "```\n",
      "----------------------------------\n",
      "Gold Standard:\n",
      " SELECT MIN(pos) FROM table_226619_12 WHERE club = \"West Ham United\"\n",
      "----------------------------------\n",
      "Took 11.548148155212402 seconds.\n",
      "====================================================================\n",
      "\n",
      "\n",
      "iter 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1612.26 ms\n",
      "llama_perf_context_print: prompt eval time =     118.79 ms /    57 tokens (    2.08 ms per token,   479.83 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3241.00 ms /    51 runs   (   63.55 ms per token,    15.74 tokens per second)\n",
      "llama_perf_context_print:       total time =    3426.07 ms /   108 tokens\n",
      "Llama.generate: 39 prefix-match hit, remaining 42 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\n",
      "SCHEMA:\n",
      "CREATE TABLE table_name_59 (premiere INTEGER, chinese_title VARCHAR, peak VARCHAR)<|im_end|>\n",
      "<|im_start|>user\n",
      "What is the premiere rating for a Chinese title of 野蠻奶奶大戰戈師奶, and a Peak smaller than 41?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Based on the provided schema and the question, the SQL query would be:\n",
      "\n",
      "```sql\n",
      "SELECT premiere\n",
      "FROM table_name_59\n",
      "WHERE chinese_title = '野蠻奶奶大戰戈師奶' AND peak < 41;\n",
      "```\n",
      "----------------------------------\n",
      "Gold Standard:\n",
      " SELECT SUM(premiere) FROM table_name_59 WHERE chinese_title = \"野蠻奶奶大戰戈師奶\" AND peak < 41\n",
      "----------------------------------\n",
      "Took 11.548148155212402 seconds.\n",
      "====================================================================\n",
      "\n",
      "\n",
      "iter 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1612.26 ms\n",
      "llama_perf_context_print: prompt eval time =      98.12 ms /    42 tokens (    2.34 ms per token,   428.06 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3549.41 ms /    56 runs   (   63.38 ms per token,    15.78 tokens per second)\n",
      "llama_perf_context_print:       total time =    3720.29 ms /    98 tokens\n",
      "Llama.generate: 39 prefix-match hit, remaining 34 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\n",
      "SCHEMA:\n",
      "CREATE TABLE table_14330096_4 (total_number VARCHAR, director VARCHAR)<|im_end|>\n",
      "<|im_start|>user\n",
      "How many of the episodes have Roger Goldby as the director?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "To find out how many episodes have Roger Goldby as the director, you can use the following SQL query:\n",
      "\n",
      "```sql\n",
      "SELECT COUNT(*) AS number_of_episodes\n",
      "FROM table_14330096_4\n",
      "WHERE director = 'Roger Goldby';\n",
      "```\n",
      "----------------------------------\n",
      "Gold Standard:\n",
      " SELECT total_number FROM table_14330096_4 WHERE director = \"Roger Goldby\"\n",
      "----------------------------------\n",
      "Took 11.548148155212402 seconds.\n",
      "====================================================================\n",
      "\n",
      "\n",
      "iter 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1612.26 ms\n",
      "llama_perf_context_print: prompt eval time =      94.50 ms /    34 tokens (    2.78 ms per token,   359.77 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1048.48 ms /    16 runs   (   65.53 ms per token,    15.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    1169.64 ms /    50 tokens\n",
      "Llama.generate: 41 prefix-match hit, remaining 42 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\n",
      "SCHEMA:\n",
      "CREATE TABLE table_name_80 (opponent VARCHAR, city VARCHAR)<|im_end|>\n",
      "<|im_start|>user\n",
      "Who was the opposing team during the game in Zagreb?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "SELECT opponent FROM table_name_80 WHERE city = 'Zagreb';\n",
      "----------------------------------\n",
      "Gold Standard:\n",
      " SELECT opponent FROM table_name_80 WHERE city = \"zagreb\"\n",
      "----------------------------------\n",
      "Took 11.548148155212402 seconds.\n",
      "====================================================================\n",
      "\n",
      "\n",
      "iter 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1612.26 ms\n",
      "llama_perf_context_print: prompt eval time =      99.00 ms /    42 tokens (    2.36 ms per token,   424.26 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6324.98 ms /    99 runs   (   63.89 ms per token,    15.65 tokens per second)\n",
      "llama_perf_context_print:       total time =    6557.16 ms /   141 tokens\n",
      "Llama.generate: 41 prefix-match hit, remaining 51 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\n",
      "SCHEMA:\n",
      "CREATE TABLE table_name_93 (total INTEGER, gold VARCHAR, silver VARCHAR)<|im_end|>\n",
      "<|im_start|>user\n",
      "What is the average total medals of the team with 2 gold and less than 1 silver?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "To find the average total medals of the team with 2 gold medals and less than 1 silver medal, you can use the following SQL query:\n",
      "\n",
      "```sql\n",
      "SELECT AVG(total) AS average_total_medals\n",
      "FROM table_name_93\n",
      "WHERE gold = '2' AND silver < '1';\n",
      "```\n",
      "\n",
      "This query calculates the average of the `total` column for rows where the `gold` column is '2' and the `silver` column is less than '1'.\n",
      "----------------------------------\n",
      "Gold Standard:\n",
      " SELECT AVG(total) FROM table_name_93 WHERE gold = 2 AND silver < 1\n",
      "----------------------------------\n",
      "Took 11.548148155212402 seconds.\n",
      "====================================================================\n",
      "\n",
      "\n",
      "iter 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1612.26 ms\n",
      "llama_perf_context_print: prompt eval time =     123.74 ms /    51 tokens (    2.43 ms per token,   412.14 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7307.93 ms /   114 runs   (   64.10 ms per token,    15.60 tokens per second)\n",
      "llama_perf_context_print:       total time =    7588.93 ms /   165 tokens\n",
      "Llama.generate: 41 prefix-match hit, remaining 30 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\n",
      "SCHEMA:\n",
      "CREATE TABLE table_name_5 (site VARCHAR, function VARCHAR, orbit VARCHAR, decay___utc__ VARCHAR)<|im_end|>\n",
      "<|im_start|>user\n",
      "What site has an orbit of Leo, a decay (UTC) of still in orbit, and a magnetosphere research?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Based on the provided schema, the query to find the site with an orbit of \"Leo\", a decay (UTC) of \"still in orbit\", and a function of \"magnetosphere research\" would be:\n",
      "\n",
      "```sql\n",
      "SELECT site\n",
      "FROM table_name_5\n",
      "WHERE orbit = 'Leo'\n",
      "  AND decay___utc__ = 'still in orbit'\n",
      "  AND function = 'magnetosphere research';\n",
      "```\n",
      "\n",
      "Please ensure that the values for `orbit`, `decay___utc__`, and `function` match exactly with what is stored in your database.\n",
      "----------------------------------\n",
      "Gold Standard:\n",
      " SELECT site FROM table_name_5 WHERE orbit = \"leo\" AND decay___utc__ = \"still in orbit\" AND function = \"magnetosphere research\"\n",
      "----------------------------------\n",
      "Took 11.548148155212402 seconds.\n",
      "====================================================================\n",
      "\n",
      "\n",
      "iter 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1612.26 ms\n",
      "llama_perf_context_print: prompt eval time =      99.78 ms /    30 tokens (    3.33 ms per token,   300.65 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2516.17 ms /    39 runs   (   64.52 ms per token,    15.50 tokens per second)\n",
      "llama_perf_context_print:       total time =    2669.82 ms /    69 tokens\n",
      "Llama.generate: 41 prefix-match hit, remaining 29 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\n",
      "SCHEMA:\n",
      "CREATE TABLE table_name_25 (opponent VARCHAR, week VARCHAR)<|im_end|>\n",
      "<|im_start|>user\n",
      "Who was the opponent in week 13?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "To find the opponent in week 13, you can use the following SQL query:\n",
      "\n",
      "```sql\n",
      "SELECT opponent\n",
      "FROM table_name_25\n",
      "WHERE week = '13';\n",
      "```\n",
      "----------------------------------\n",
      "Gold Standard:\n",
      " SELECT opponent FROM table_name_25 WHERE week = 13\n",
      "----------------------------------\n",
      "Took 11.548148155212402 seconds.\n",
      "====================================================================\n",
      "\n",
      "\n",
      "iter 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1612.26 ms\n",
      "llama_perf_context_print: prompt eval time =      99.80 ms /    29 tokens (    3.44 ms per token,   290.58 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4629.52 ms /    72 runs   (   64.30 ms per token,    15.55 tokens per second)\n",
      "llama_perf_context_print:       total time =    4831.83 ms /   101 tokens\n",
      "Llama.generate: 41 prefix-match hit, remaining 38 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\n",
      "SCHEMA:\n",
      "CREATE TABLE table_name_78 (moving_to VARCHAR, nat VARCHAR)<|im_end|>\n",
      "<|im_start|>user\n",
      "Who is moving with the nationality of Tri?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "To find out who is moving with the nationality of \"Tri\", you can use the following SQL query:\n",
      "\n",
      "```sql\n",
      "SELECT moving_to\n",
      "FROM table_name_78\n",
      "WHERE nat = 'Tri';\n",
      "```\n",
      "\n",
      "This query selects the `moving_to` column from `table_name_78` where the `nat` column is equal to 'Tri'.\n",
      "----------------------------------\n",
      "Gold Standard:\n",
      " SELECT moving_to FROM table_name_78 WHERE nat = \"tri\"\n",
      "----------------------------------\n",
      "Took 11.548148155212402 seconds.\n",
      "====================================================================\n",
      "\n",
      "\n",
      "iter 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1612.26 ms\n",
      "llama_perf_context_print: prompt eval time =      98.61 ms /    38 tokens (    2.59 ms per token,   385.37 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6028.35 ms /    94 runs   (   64.13 ms per token,    15.59 tokens per second)\n",
      "llama_perf_context_print:       total time =    6252.34 ms /   132 tokens\n",
      "Llama.generate: 41 prefix-match hit, remaining 36 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\n",
      "SCHEMA:\n",
      "CREATE TABLE table_name_87 (game_site VARCHAR, date VARCHAR)<|im_end|>\n",
      "<|im_start|>user\n",
      "Which game site hosted a match on April 10, 2004?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "To find out which game site hosted a match on April 10, 2004, you can use the following SQL query:\n",
      "\n",
      "```sql\n",
      "SELECT game_site\n",
      "FROM table_name_87\n",
      "WHERE date = '2004-04-10';\n",
      "```\n",
      "\n",
      "This query selects the `game_site` from `table_name_87` where the `date` matches '2004-04-10'.\n",
      "----------------------------------\n",
      "Gold Standard:\n",
      " SELECT game_site FROM table_name_87 WHERE date = \"april 10, 2004\"\n",
      "----------------------------------\n",
      "Took 11.548148155212402 seconds.\n",
      "====================================================================\n",
      "\n",
      "\n",
      "iter 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1612.26 ms\n",
      "llama_perf_context_print: prompt eval time =      98.59 ms /    36 tokens (    2.74 ms per token,   365.15 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3735.21 ms /    58 runs   (   64.40 ms per token,    15.53 tokens per second)\n",
      "llama_perf_context_print:       total time =    3916.03 ms /    94 tokens\n",
      "Llama.generate: 41 prefix-match hit, remaining 33 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\n",
      "SCHEMA:\n",
      "CREATE TABLE table_name_37 (date VARCHAR, result VARCHAR, venue VARCHAR)<|im_end|>\n",
      "<|im_start|>user\n",
      "Name the date for result of draw, and venue of edgbaston<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Based on the provided schema, the SQL query to retrieve the date for results of 'draw' and venue of 'edgbaston' would be:\n",
      "\n",
      "```sql\n",
      "SELECT date\n",
      "FROM table_name_37\n",
      "WHERE result = 'draw' AND venue = 'edgbaston';\n",
      "```\n",
      "----------------------------------\n",
      "Gold Standard:\n",
      " SELECT date FROM table_name_37 WHERE result = \"draw\" AND venue = \"edgbaston\"\n",
      "----------------------------------\n",
      "Took 11.548148155212402 seconds.\n",
      "====================================================================\n",
      "\n",
      "\n",
      "iter 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1612.26 ms\n",
      "llama_perf_context_print: prompt eval time =      99.12 ms /    33 tokens (    3.00 ms per token,   332.91 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5195.69 ms /    81 runs   (   64.14 ms per token,    15.59 tokens per second)\n",
      "llama_perf_context_print:       total time =    5403.53 ms /   114 tokens\n",
      "Llama.generate: 41 prefix-match hit, remaining 49 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\n",
      "SCHEMA:\n",
      "CREATE TABLE table_name_48 (location VARCHAR, attendance VARCHAR)<|im_end|>\n",
      "<|im_start|>user\n",
      "At what Location was the Attendance 70,283?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "To find the location where the attendance was 70,283, you can use the following SQL query:\n",
      "\n",
      "```sql\n",
      "SELECT location\n",
      "FROM table_name_48\n",
      "WHERE attendance = '70,283';\n",
      "```\n",
      "\n",
      "This query selects the `location` from `table_name_48` where the `attendance` is equal to '70,283'.\n",
      "----------------------------------\n",
      "Gold Standard:\n",
      " SELECT location FROM table_name_48 WHERE attendance = \"70,283\"\n",
      "----------------------------------\n",
      "Took 11.548148155212402 seconds.\n",
      "====================================================================\n",
      "\n",
      "\n",
      "iter 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1612.26 ms\n",
      "llama_perf_context_print: prompt eval time =     122.45 ms /    49 tokens (    2.50 ms per token,   400.16 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7436.69 ms /   116 runs   (   64.11 ms per token,    15.60 tokens per second)\n",
      "llama_perf_context_print:       total time =    7721.36 ms /   165 tokens\n",
      "Llama.generate: 39 prefix-match hit, remaining 47 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\n",
      "SCHEMA:\n",
      "CREATE TABLE table_name_18 (podiums INTEGER, class VARCHAR, f_laps VARCHAR)<|im_end|>\n",
      "<|im_start|>user\n",
      "How many Podiums have a Class of 250cc, and an F laps of 0?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Based on the provided schema, the SQL query to find the number of Podiums with a Class of '250cc' and F laps of '0' would be:\n",
      "\n",
      "```sql\n",
      "SELECT COUNT(*) AS number_of_podiums\n",
      "FROM table_name_18\n",
      "WHERE class = '250cc' AND f_laps = '0';\n",
      "```\n",
      "\n",
      "This query counts the number of rows in `table_name_18` where the `class` column is '250cc' and the `f_laps` column is '0'.\n",
      "----------------------------------\n",
      "Gold Standard:\n",
      " SELECT SUM(podiums) FROM table_name_18 WHERE class = \"250cc\" AND f_laps = 0\n",
      "----------------------------------\n",
      "Took 11.548148155212402 seconds.\n",
      "====================================================================\n",
      "\n",
      "\n",
      "iter 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1612.26 ms\n",
      "llama_perf_context_print: prompt eval time =     107.85 ms /    47 tokens (    2.29 ms per token,   435.77 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7011.53 ms /   109 runs   (   64.33 ms per token,    15.55 tokens per second)\n",
      "llama_perf_context_print:       total time =    7269.09 ms /   156 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\n",
      "SCHEMA:\n",
      "CREATE TABLE table_30120560_1 (civil_parish VARCHAR, area__acres__ VARCHAR)<|im_end|>\n",
      "<|im_start|>user\n",
      "which cilvil parishes have areas of 405?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Based on the provided schema, the SQL query to find civil parishes with an area of 405 acres would be:\n",
      "\n",
      "```sql\n",
      "SELECT civil_parish\n",
      "FROM table_30120560_1\n",
      "WHERE area__acres__ = '405';\n",
      "```\n",
      "\n",
      "Note: The `area__acres__` column is of type `VARCHAR`, so the value '405' is enclosed in single quotes. If the column were of a numeric type, the quotes would not be necessary.\n",
      "----------------------------------\n",
      "Gold Standard:\n",
      " SELECT civil_parish FROM table_30120560_1 WHERE area__acres__ = 405\n",
      "----------------------------------\n",
      "Took 11.548148155212402 seconds.\n",
      "====================================================================\n",
      "\n",
      "\n",
      "998 Tue Feb  4 13:50:32 2025\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# random sample without replacement\n",
    "import random\n",
    "random.seed(10)\n",
    "numbers = random.sample(range(0, 9999), 20)\n",
    "print('numbers',numbers)\n",
    "#\n",
    "mssgs101 = dataset[numbers]\n",
    "\n",
    "print('====================================================================\\n\\n')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print( '22' , time.asctime( time.localtime( time.time() ) ))\n",
    "\n",
    "\n",
    "# ini\n",
    "GSsL = []\n",
    "LmOutputsL = []\n",
    "llmrspnstxtL = []\n",
    "TimesL =[]\n",
    "\n",
    "#\n",
    "for cntr,valu in enumerate(mssgs101['messages']):\n",
    "    print('iter', cntr)\n",
    "    LlmSysPrmpt = valu[0]['content']\n",
    "    LlmQstn = valu[1]['content']\n",
    "    GoldStd = valu[2]['content']\n",
    "    \n",
    "    #\n",
    "    prmpt56 = \"\"\"<|im_start|>system\n",
    "\"\"\"+LlmSysPrmpt+\"\"\"<|im_end|>\n",
    "<|im_start|>user\n",
    "\"\"\"+LlmQstn+\"\"\"<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "    #\n",
    "    startt00 = time.time()\n",
    "    #\n",
    "    output11 = llm101.create_completion(\n",
    "        prmpt56,\n",
    "        max_tokens=999,\n",
    "        stop=[\"Q:\"], \n",
    "        temperature=0.0,\n",
    "        #cache_prompt=False, # TypeError: Llama.create_completion() got an unexpected keyword argument 'cache_prompt' (https://github.com/ggerganov/llama.cpp/issues/4288)\n",
    "        top_k=1, # top_k (int, default: 40 ) – The top-k value to use for sampling. Top-K sampling described in academic paper \"The Curious Case of Neural Text Degeneration\" https://arxiv.org/abs/1904.09751\n",
    "        echo=True # Echo the prompt back in the output\n",
    "    )\n",
    "    #\n",
    "    endt00 = time.time()\n",
    "    TimeTookE2E = end00 - start00\n",
    "    #\n",
    "    llmrspnstxt = output11['choices'][0]['text']\n",
    "    #\n",
    "    GSsL.append(GoldStd)\n",
    "    LmOutputsL.append(output11)\n",
    "    llmrspnstxtL.append(llmrspnstxt)\n",
    "    TimesL.append(TimeTookE2E)\n",
    "    #\n",
    "    print(llmrspnstxt)\n",
    "    print('----------------------------------')\n",
    "    print('Gold Standard:\\n', GoldStd)\n",
    "    print('----------------------------------')\n",
    "    print(\"Took {} seconds.\".format(TimeTookE2E))\n",
    "    print('====================================================================\\n\\n')\n",
    "#\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print( '998' , time.asctime( time.localtime( time.time() ) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a941421d-1cb6-421d-afea-d91a48970269",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830f9bac-8639-485a-b8e9-06c0f49f068b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119542a6-0982-4e00-babf-5a77832e64b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3ae4b66e-6197-4625-b59e-776b919940a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "228\n",
      "['absl-py==2.1.0', 'anyio==3.7.0', 'apache-beam==2.53.0', 'appdirs==1.4.4', 'apsw==3.36.0.post1', 'apturl==0.5.2', 'argon2-cffi==21.3.0', 'argon2-cffi-bindings==21.2.0', 'arrow==1.2.3', 'asttokens==2.2.1', 'async-lru==2.0.2', 'attrs==23.1.0', 'Babel==2.12.1', 'backcall==0.2.0', 'bcrypt==3.2.0', 'beautifulsoup4==4.12.2', 'beniget==0.4.1', 'bleach==6.0.0', 'blinker==1.4', 'Brlapi==0.8.3', 'Brotli==1.0.9', 'certifi==2020.6.20', 'cffi==1.15.1', 'chardet==4.0.0', 'charset-normalizer==3.1.0', 'click==8.0.3', 'cloudpickle==2.2.1', 'colorama==0.4.4', 'comm==0.1.3', 'command-not-found==0.3', 'crcmod==1.7', 'cryptography==3.4.8', 'css-parser==1.0.7', 'cssselect==1.1.0', 'cssutils==1.0.2', 'cupshelpers==1.0', 'cycler==0.11.0', 'dacite==1.8.1', 'dbus-python==1.2.18', 'debugpy==1.6.7', 'decorator==5.1.1', 'defer==1.0.6', 'defusedxml==0.7.1', 'dill==0.3.1.1', 'distro==1.7.0', 'distro-info==1.1+ubuntu0.2', 'dnspython==2.5.0', 'docopt==0.6.2', 'duplicity==0.8.21', 'exceptiongroup==1.1.2', 'executing==1.2.0', 'fastavro==1.9.3', 'fasteners==0.14.1', 'fastjsonschema==2.17.1', 'feedparser==6.0.8', 'fonttools==4.29.1', 'fqdn==1.5.1', 'fs==2.4.12', 'future==0.18.2', 'gast==0.5.2', 'grpcio==1.60.1', 'h11==0.14.0', 'hdfs==2.7.3', 'html2text==2020.1.16', 'html5-parser==0.4.10', 'html5lib==1.1', 'httpcore==1.0.4', 'httplib2==0.20.2', 'httpx==0.27.0', 'idna==3.3', 'ifaddr==0.1.7', 'importlib-metadata==4.6.4', 'ipykernel==6.24.0', 'ipython==8.14.0', 'isoduration==20.11.0', 'jedi==0.18.2', 'jeepney==0.7.1', 'Jinja2==3.1.2', 'Js2Py==0.74', 'json5==0.9.14', 'jsonpickle==3.0.2', 'jsonpointer==2.4', 'jsonschema==4.23.0', 'jsonschema-specifications==2023.12.1', 'jupyter-events==0.6.3', 'jupyter-lsp==2.2.0', 'jupyter_client==8.3.0', 'jupyter_core==5.3.1', 'jupyter_server==2.7.0', 'jupyter_server_terminals==0.4.4', 'jupyterlab==4.3.5', 'jupyterlab-pygments==0.2.2', 'jupyterlab_server==2.27.3', 'keyring==23.5.0', 'kiwisolver==1.3.2', 'language-selector==0.1', 'launchpadlib==1.10.16', 'lazr.restfulclient==0.14.4', 'lazr.uri==1.0.6', 'lockfile==0.12.2', 'louis==3.20.0', 'lxml==4.8.0', 'lz4==3.1.3+dfsg', 'macaroonbakery==1.3.1', 'Mako==1.1.3', 'Markdown==3.3.6', 'MarkupSafe==2.0.1', 'matplotlib==3.5.1', 'matplotlib-inline==0.1.6', 'mechanize==0.4.7', 'mistune==3.0.1', 'monotonic==1.6', 'more-itertools==8.10.0', 'mpmath==0.0.0', 'msgpack==1.0.3', 'mt-metrics-eval @ file:///home/ghtw30s/virenv20231031/mt-metrics-eval', 'nbclient==0.8.0', 'nbconvert==7.6.0', 'nbformat==5.9.0', 'nest-asyncio==1.5.6', 'netifaces==0.11.0', 'notebook_shim==0.2.3', 'numpy==1.24.4', 'oauthlib==3.2.0', 'objsize==0.6.1', 'olefile==0.46', 'orjson==3.9.12', 'overrides==7.3.1', 'packaging==23.1', 'pandocfilters==1.5.0', 'paramiko==2.9.3', 'parso==0.8.3', 'pexpect==4.8.0', 'pickleshare==0.7.5', 'Pillow==9.0.1', 'pip==24.0', 'platformdirs==3.8.0', 'ply==3.11', 'prometheus-client==0.17.0', 'prompt-toolkit==3.0.39', 'proto-plus==1.23.0', 'protobuf==4.25.2', 'psutil==5.9.5', 'ptyprocess==0.7.0', 'pure-eval==0.2.2', 'py7zr==0.11.3+dfsg', 'pyarrow==14.0.2', 'pyarrow-hotfix==0.6', 'pycairo==1.20.1', 'pychm==0.8.6', 'pycparser==2.21', 'pycryptodomex==3.11.0', 'pycups==2.0.1', 'pydot==1.4.2', 'Pygments==2.15.1', 'PyGObject==3.42.1', 'pyjsparser==2.7.1', 'PyJWT==2.3.0', 'pymacaroons==0.13.0', 'pymongo==4.6.1', 'PyNaCl==1.5.0', 'pyparsing==2.4.7', 'PyQt5==5.15.6', 'PyQt5-sip==12.9.1', 'PyQtWebEngine==5.15.5', 'pyRFC3339==1.1', 'pyrsistent==0.19.3', 'python-apt==2.4.0+ubuntu4', 'python-dateutil==2.8.2', 'python-debian==0.1.43+ubuntu1.1', 'python-json-logger==2.0.7', 'pythran==0.10.0', 'pytz==2022.1', 'pyxdg==0.27', 'PyYAML==5.4.1', 'pyzmq==25.1.0', 'referencing==0.35.1', 'regex==2023.12.25', 'reportlab==3.6.8', 'repoze.lru==0.7', 'requests==2.31.0', 'requests-toolbelt==0.9.1', 'rfc3339-validator==0.1.4', 'rfc3986-validator==0.1.1', 'Routes==2.5.1', 'rpds-py==0.19.0', 'scipy==1.12.0', 'screen-resolution-extra==0.0.0', 'SecretStorage==3.3.1', 'Send2Trash==1.8.2', 'setuptools==59.6.0', 'sgmllib3k==1.0.0', 'six==1.16.0', 'sniffio==1.3.0', 'soupsieve==2.4.1', 'ssh-import-id==5.11', 'stack-data==0.6.2', 'sympy==1.9', 'systemd-python==234', 'terminado==0.17.1', 'texttable==1.6.4', 'tinycss2==1.2.1', 'tomli==2.0.1', 'tornado==6.3.2', 'traitlets==5.9.0', 'typing_extensions==4.7.1', 'tzlocal==5.2', 'ubuntu-drivers-common==0.0.0', 'ubuntu-pro-client==8001', 'ufoLib2==0.13.1', 'ufw==0.36.1', 'unattended-upgrades==0.1', 'unicodedata2==14.0.0', 'uri-template==1.3.0', 'urllib3==1.26.5', 'usb-creator==0.3.7', 'wadllib==1.3.6', 'wcwidth==0.2.6', 'webcolors==24.6.0', 'webencodings==0.5.1', 'WebOb==1.8.6', 'websocket-client==1.6.1', 'wheel==0.37.1', 'xdg==5', 'xkit==0.0.0', 'zeroconf==0.38.3', 'zipp==1.0.0', 'zstandard==0.22.0'] \n",
      "\n",
      "absl-py==2.1.0\n",
      "anyio==3.7.0\n",
      "apache-beam==2.53.0\n",
      "appdirs==1.4.4\n",
      "apsw==3.36.0.post1\n",
      "apturl==0.5.2\n",
      "argon2-cffi==21.3.0\n",
      "argon2-cffi-bindings==21.2.0\n",
      "arrow==1.2.3\n",
      "asttokens==2.2.1\n",
      "async-lru==2.0.2\n",
      "attrs==23.1.0\n",
      "Babel==2.12.1\n",
      "backcall==0.2.0\n",
      "bcrypt==3.2.0\n",
      "beautifulsoup4==4.12.2\n",
      "beniget==0.4.1\n",
      "bleach==6.0.0\n",
      "blinker==1.4\n",
      "Brlapi==0.8.3\n",
      "Brotli==1.0.9\n",
      "certifi==2020.6.20\n",
      "cffi==1.15.1\n",
      "chardet==4.0.0\n",
      "charset-normalizer==3.1.0\n",
      "click==8.0.3\n",
      "cloudpickle==2.2.1\n",
      "colorama==0.4.4\n",
      "comm==0.1.3\n",
      "command-not-found==0.3\n",
      "crcmod==1.7\n",
      "cryptography==3.4.8\n",
      "css-parser==1.0.7\n",
      "cssselect==1.1.0\n",
      "cssutils==1.0.2\n",
      "cupshelpers==1.0\n",
      "cycler==0.11.0\n",
      "dacite==1.8.1\n",
      "dbus-python==1.2.18\n",
      "debugpy==1.6.7\n",
      "decorator==5.1.1\n",
      "defer==1.0.6\n",
      "defusedxml==0.7.1\n",
      "dill==0.3.1.1\n",
      "distro==1.7.0\n",
      "distro-info==1.1+ubuntu0.2\n",
      "dnspython==2.5.0\n",
      "docopt==0.6.2\n",
      "duplicity==0.8.21\n",
      "exceptiongroup==1.1.2\n",
      "executing==1.2.0\n",
      "fastavro==1.9.3\n",
      "fasteners==0.14.1\n",
      "fastjsonschema==2.17.1\n",
      "feedparser==6.0.8\n",
      "fonttools==4.29.1\n",
      "fqdn==1.5.1\n",
      "fs==2.4.12\n",
      "future==0.18.2\n",
      "gast==0.5.2\n",
      "grpcio==1.60.1\n",
      "h11==0.14.0\n",
      "hdfs==2.7.3\n",
      "html2text==2020.1.16\n",
      "html5-parser==0.4.10\n",
      "html5lib==1.1\n",
      "httpcore==1.0.4\n",
      "httplib2==0.20.2\n",
      "httpx==0.27.0\n",
      "idna==3.3\n",
      "ifaddr==0.1.7\n",
      "importlib-metadata==4.6.4\n",
      "ipykernel==6.24.0\n",
      "ipython==8.14.0\n",
      "isoduration==20.11.0\n",
      "jedi==0.18.2\n",
      "jeepney==0.7.1\n",
      "Jinja2==3.1.2\n",
      "Js2Py==0.74\n",
      "json5==0.9.14\n",
      "jsonpickle==3.0.2\n",
      "jsonpointer==2.4\n",
      "jsonschema==4.23.0\n",
      "jsonschema-specifications==2023.12.1\n",
      "jupyter-events==0.6.3\n",
      "jupyter-lsp==2.2.0\n",
      "jupyter_client==8.3.0\n",
      "jupyter_core==5.3.1\n",
      "jupyter_server==2.7.0\n",
      "jupyter_server_terminals==0.4.4\n",
      "jupyterlab==4.3.5\n",
      "jupyterlab-pygments==0.2.2\n",
      "jupyterlab_server==2.27.3\n",
      "keyring==23.5.0\n",
      "kiwisolver==1.3.2\n",
      "language-selector==0.1\n",
      "launchpadlib==1.10.16\n",
      "lazr.restfulclient==0.14.4\n",
      "lazr.uri==1.0.6\n",
      "lockfile==0.12.2\n",
      "louis==3.20.0\n",
      "lxml==4.8.0\n",
      "lz4==3.1.3+dfsg\n",
      "macaroonbakery==1.3.1\n",
      "Mako==1.1.3\n",
      "Markdown==3.3.6\n",
      "MarkupSafe==2.0.1\n",
      "matplotlib==3.5.1\n",
      "matplotlib-inline==0.1.6\n",
      "mechanize==0.4.7\n",
      "mistune==3.0.1\n",
      "monotonic==1.6\n",
      "more-itertools==8.10.0\n",
      "mpmath==0.0.0\n",
      "msgpack==1.0.3\n",
      "mt-metrics-eval @ file:///home/ghtw30s/virenv20231031/mt-metrics-eval\n",
      "nbclient==0.8.0\n",
      "nbconvert==7.6.0\n",
      "nbformat==5.9.0\n",
      "nest-asyncio==1.5.6\n",
      "netifaces==0.11.0\n",
      "notebook_shim==0.2.3\n",
      "numpy==1.24.4\n",
      "oauthlib==3.2.0\n",
      "objsize==0.6.1\n",
      "olefile==0.46\n",
      "orjson==3.9.12\n",
      "overrides==7.3.1\n",
      "packaging==23.1\n",
      "pandocfilters==1.5.0\n",
      "paramiko==2.9.3\n",
      "parso==0.8.3\n",
      "pexpect==4.8.0\n",
      "pickleshare==0.7.5\n",
      "Pillow==9.0.1\n",
      "pip==24.0\n",
      "platformdirs==3.8.0\n",
      "ply==3.11\n",
      "prometheus-client==0.17.0\n",
      "prompt-toolkit==3.0.39\n",
      "proto-plus==1.23.0\n",
      "protobuf==4.25.2\n",
      "psutil==5.9.5\n",
      "ptyprocess==0.7.0\n",
      "pure-eval==0.2.2\n",
      "py7zr==0.11.3+dfsg\n",
      "pyarrow==14.0.2\n",
      "pyarrow-hotfix==0.6\n",
      "pycairo==1.20.1\n",
      "pychm==0.8.6\n",
      "pycparser==2.21\n",
      "pycryptodomex==3.11.0\n",
      "pycups==2.0.1\n",
      "pydot==1.4.2\n",
      "Pygments==2.15.1\n",
      "PyGObject==3.42.1\n",
      "pyjsparser==2.7.1\n",
      "PyJWT==2.3.0\n",
      "pymacaroons==0.13.0\n",
      "pymongo==4.6.1\n",
      "PyNaCl==1.5.0\n",
      "pyparsing==2.4.7\n",
      "PyQt5==5.15.6\n",
      "PyQt5-sip==12.9.1\n",
      "PyQtWebEngine==5.15.5\n",
      "pyRFC3339==1.1\n",
      "pyrsistent==0.19.3\n",
      "python-apt==2.4.0+ubuntu4\n",
      "python-dateutil==2.8.2\n",
      "python-debian==0.1.43+ubuntu1.1\n",
      "python-json-logger==2.0.7\n",
      "pythran==0.10.0\n",
      "pytz==2022.1\n",
      "pyxdg==0.27\n",
      "PyYAML==5.4.1\n",
      "pyzmq==25.1.0\n",
      "referencing==0.35.1\n",
      "regex==2023.12.25\n",
      "reportlab==3.6.8\n",
      "repoze.lru==0.7\n",
      "requests==2.31.0\n",
      "requests-toolbelt==0.9.1\n",
      "rfc3339-validator==0.1.4\n",
      "rfc3986-validator==0.1.1\n",
      "Routes==2.5.1\n",
      "rpds-py==0.19.0\n",
      "scipy==1.12.0\n",
      "screen-resolution-extra==0.0.0\n",
      "SecretStorage==3.3.1\n",
      "Send2Trash==1.8.2\n",
      "setuptools==59.6.0\n",
      "sgmllib3k==1.0.0\n",
      "six==1.16.0\n",
      "sniffio==1.3.0\n",
      "soupsieve==2.4.1\n",
      "ssh-import-id==5.11\n",
      "stack-data==0.6.2\n",
      "sympy==1.9\n",
      "systemd-python==234\n",
      "terminado==0.17.1\n",
      "texttable==1.6.4\n",
      "tinycss2==1.2.1\n",
      "tomli==2.0.1\n",
      "tornado==6.3.2\n",
      "traitlets==5.9.0\n",
      "typing_extensions==4.7.1\n",
      "tzlocal==5.2\n",
      "ubuntu-drivers-common==0.0.0\n",
      "ubuntu-pro-client==8001\n",
      "ufoLib2==0.13.1\n",
      "ufw==0.36.1\n",
      "unattended-upgrades==0.1\n",
      "unicodedata2==14.0.0\n",
      "uri-template==1.3.0\n",
      "urllib3==1.26.5\n",
      "usb-creator==0.3.7\n",
      "wadllib==1.3.6\n",
      "wcwidth==0.2.6\n",
      "webcolors==24.6.0\n",
      "webencodings==0.5.1\n",
      "WebOb==1.8.6\n",
      "websocket-client==1.6.1\n",
      "wheel==0.37.1\n",
      "xdg==5\n",
      "xkit==0.0.0\n",
      "zeroconf==0.38.3\n",
      "zipp==1.0.0\n",
      "zstandard==0.22.0\n",
      "Tue Feb  4 01:50:33 PM EST 2025\n"
     ]
    }
   ],
   "source": [
    "rslt5 = !pip freeze --all\n",
    "print(len(rslt5))\n",
    "print(rslt5 , \"\\n\")\n",
    "#print(\"\\n\".join(rslt5) )\n",
    "print( *rslt5, sep=\"\\n\" )\n",
    "!date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6472dd3-6f54-4a32-a4c6-4059ffd7ae30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virenv20240420j",
   "language": "python",
   "name": "virenv20240420j"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
